{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "import gym\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import more_itertools as mitt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [12, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v1')\n",
    "torch.cuda.current_device()\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "  device = \"cuda:0\" \n",
    "else:  \n",
    "  device = \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = namedtuple(\n",
    "    'Batch', ('states', 'actions', 'rewards', 'next_states', 'dones')\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, max_size, state_size, action_size):\n",
    "        self.max_size = max_size\n",
    "        self.state_size = state_size\n",
    "        self.states = torch.empty((max_size, state_size), device=device)\n",
    "        self.actions = torch.empty((max_size, action_size), device=device)\n",
    "        self.rewards = torch.empty((max_size, 1), device=device)\n",
    "        self.next_states = torch.empty((max_size, state_size), device=device)\n",
    "        self.dones = torch.empty((max_size, 1), dtype=torch.bool, device=device)\n",
    "        self.idx = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states[self.idx] = torch.tensor(state, device=device)\n",
    "        self.actions[self.idx] = torch.tensor(action, device=device)\n",
    "        self.rewards[self.idx] = torch.tensor(reward, device=device)\n",
    "        self.next_states[self.idx] = torch.tensor(next_state, device=device)\n",
    "        self.dones[self.idx] = torch.tensor(done, device=device)\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size) -> Batch:\n",
    "        if self.size <= batch_size:\n",
    "            sample_indices = np.random.choice(self.size, self.size, replace=False)\n",
    "        else:\n",
    "            sample_indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        batch = Batch(\n",
    "                        states = self.states[ sample_indices ,:],\n",
    "                        actions = self.actions[ sample_indices ,:],\n",
    "                        rewards = self.rewards[ sample_indices ,:],\n",
    "                        next_states = self.next_states[ sample_indices ,:],\n",
    "                        dones = self.dones[ sample_indices ,:]\n",
    "                    )\n",
    "        return batch\n",
    "\n",
    "    def populate(self, env, num_steps):\n",
    "        state = env.reset()\n",
    "        state = np.concatenate((state[\"observation\"], state[\"achieved_goal\"], state[\"desired_goal\"]), axis=0)\n",
    "        for i in range(num_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.concatenate((next_state[\"observation\"], next_state[\"achieved_goal\"], next_state[\"desired_goal\"]), axis=0)\n",
    "            self.add(state, action, reward, next_state, done)\n",
    "            if i != 0 and i%10000 == 0:\n",
    "                print(i)\n",
    "            if done:\n",
    "                if not('TimeLimit.truncated' in info):\n",
    "                    print(next_state, reward, done, info, i)\n",
    "                state = env.reset()\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers, units=256):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.units = units\n",
    "        \n",
    "        self.layers = nn.ModuleList([nn.Linear(self.input_size, self.units)])\n",
    "        self.layers.extend([ nn.Linear(self.units, self.units) for i in range(1, self.hidden_layers) ])\n",
    "        self.layers.append(nn.Linear(self.units, self.output_size))\n",
    "    \n",
    "    def forward(self, states):\n",
    "        vals = states\n",
    "        for layer_index in range(len(self.layers) - 1):\n",
    "            vals = F.relu(self.layers[layer_index](vals))\n",
    "        vals = torch.tanh(self.layers[layer_index + 1](vals))\n",
    "        return vals\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers, units=256):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.units = units\n",
    "        \n",
    "        self.layers = nn.ModuleList([nn.Linear(self.input_size, self.units)])\n",
    "        self.layers.extend([ nn.Linear(self.units, self.units) for i in range(1, self.hidden_layers) ])\n",
    "        self.layers.append(nn.Linear(self.units, self.output_size))\n",
    "    \n",
    "    def forward(self, states, actions):\n",
    "        vals = torch.cat([states, actions], 1)\n",
    "        for layer_index in range(len(self.layers) - 1):\n",
    "            vals = F.relu(self.layers[layer_index](vals))\n",
    "        vals = self.layers[layer_index + 1](vals)\n",
    "        return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise(object):\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[0]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_noisy_action(self, action, t=0): \n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, actor_learning_rate=0.0001, critic_learning_rate=0.0001, gamma=0.99, tau=0.005):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.actor_lr = actor_learning_rate\n",
    "        self.critic_lr = critic_learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.actor = Actor(self.state_size, self.action_size, 5)\n",
    "        self.target_actor = Actor(self.state_size, self.action_size, 5)\n",
    "        \n",
    "        self.critic = Critic(self.state_size + self.action_size, self.action_size, 5)\n",
    "        self.target_critic = Critic(self.state_size + self.action_size, self.action_size, 5)\n",
    "    \n",
    "        self.actor.to(device)\n",
    "        self.target_actor.to(device)\n",
    "        self.critic.to(device)\n",
    "        self.target_critic.to(device)\n",
    "        self.update_weights(1)\n",
    "        \n",
    "        self.critic_loss_method  = nn.MSELoss()\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "        # print(next(self.actor.parameters()).is_cuda)\n",
    "    \n",
    "    def update_weights(self, tau):\n",
    "        for target_weights, weights in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_weights.data.copy_(weights.data * tau + target_weights.data * (1.0 - tau))\n",
    "        \n",
    "        for target_weights, weights in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_weights.data.copy_(weights.data * tau + target_weights.data * (1.0 - tau))\n",
    "            \n",
    "    def get_actions(self, states):\n",
    "        return self.actor.forward(states).detach()\n",
    "\n",
    "    def train_batch(self, states, actions, rewards, next_states, dones):\n",
    "        #print(dones)\n",
    "        \n",
    "        Q_vals = self.critic(states, actions)\n",
    "        next_actions = self.target_actor(next_states)\n",
    "        next_actions = next_actions.detach()\n",
    "        Q_dash = self.target_critic(next_states, next_actions)\n",
    "        dones = torch.logical_not(dones).float()\n",
    "        Q_dash = rewards + self.gamma * Q_dash * dones\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss = self.critic_loss_method(Q_vals, Q_dash)\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss = - self.critic(states, self.actor.forward(states)).mean()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.update_weights(self.tau)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        self.target_actor.load_state_dict(torch.load(model_path))\n",
    "        self.target_actor.eval()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cd06676a4245faa77ccb00ccfaafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=1500000), HTML(value='')), layout=Layout(disp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-fbf53868b882>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt_total\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_total\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mexperience_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt_total\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_total\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1_00_000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-a77689a5b380>\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_loss_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_dash\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mcritic_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TIMESTEPS = 15_00_000\n",
    "BATCH_SIZE = 64\n",
    "state_dim = 16\n",
    "action_dim = 4\n",
    "memory_size = 1_00_000\n",
    "replay_memory = ReplayMemory(memory_size, state_dim, action_dim)\n",
    "replay_memory.populate(env, 50000)\n",
    "saved_models = {}\n",
    "count = 0\n",
    "ou = OUNoise(env.action_space)\n",
    "ddpg = DDPG(state_dim, action_dim)\n",
    "\n",
    "state = env.reset()\n",
    "state = np.concatenate((state[\"observation\"], state[\"achieved_goal\"], state[\"desired_goal\"]), axis=0)\n",
    "pbar = tqdm.tnrange(TIMESTEPS, ncols='100%')\n",
    "for t_total in pbar:\n",
    "    cuda_state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "    action = ddpg.get_actions(cuda_state).cpu()\n",
    "    action = ou.get_noisy_action(action[0].numpy())\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = np.concatenate((next_state[\"observation\"], next_state[\"achieved_goal\"], next_state[\"desired_goal\"]), axis=0)\n",
    "    replay_memory.add(state, action, reward, next_state, done)\n",
    "    if t_total != 0 and t_total % 4 == 0:\n",
    "        experience_batch = replay_memory.sample(BATCH_SIZE)\n",
    "        ddpg.train_batch(experience_batch.states, experience_batch.actions, experience_batch.rewards, experience_batch.next_states, experience_batch.dones)\n",
    "    \n",
    "    if t_total != 0 and t_total % 1_00_000 == 0:\n",
    "        torch.save(ddpg.target_actor.state_dict(), \"mc_\" + str(count))\n",
    "        count += 1\n",
    "        print(\"saved\")\n",
    "            \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        state = np.concatenate((state[\"observation\"], state[\"achieved_goal\"], state[\"desired_goal\"]), axis=0)\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "torch.save(ddpg.target_actor.state_dict(), \"mc_\" + str(count))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c3959a9fb543e9a146bd452275ec47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=1), HTML(value='')), layout=Layout(display='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "{'state': {'observation': array([1.43554725e+00, 8.40139887e-01, 4.36115517e-01, 0.00000000e+00,\n",
      "       0.00000000e+00, 1.20485296e-02, 2.11136234e-02, 2.22120075e-03,\n",
      "       4.34532739e-04, 1.12561556e-04]), 'achieved_goal': array([1.43554725, 0.84013989, 0.43611552]), 'desired_goal': array([1.43554725, 0.84013989, 0.43611552])}, 'action': array([0.42742689, 0.96687077, 0.41928199, 0.80452785]), 'reward': -1.0, 'next_state': {'observation': array([1.44859844e+00, 8.72084490e-01, 4.49256573e-01, 0.00000000e+00,\n",
      "       0.00000000e+00, 9.19822039e-03, 2.28496478e-02, 1.07143146e-02,\n",
      "       4.79949917e-04, 1.63879854e-05]), 'achieved_goal': array([1.44859844, 0.87208449, 0.44925657]), 'desired_goal': array([1.43554725, 0.84013989, 0.43611552])}, 'done': False}\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "{'state': {'observation': array([ 1.46511672e+00,  8.92880917e-01,  4.64175041e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.29720847e-02,  1.31319100e-02,  1.04607027e-02,\n",
      "        6.70301660e-04, -3.45699807e-06]), 'achieved_goal': array([1.46511672, 0.89288092, 0.46417504]), 'desired_goal': array([1.46511672, 0.89288092, 0.46417504])}, 'action': array([0.50311599, 0.80493425, 0.52265443, 0.94091546]), 'reward': -1.0, 'next_state': {'observation': array([1.47864849e+00, 9.18482417e-01, 4.81316280e-01, 0.00000000e+00,\n",
      "       0.00000000e+00, 9.45475429e-03, 1.90991360e-02, 1.24116848e-02,\n",
      "       3.77201621e-04, 7.83891799e-05]), 'achieved_goal': array([1.47864849, 0.91848242, 0.48131628]), 'desired_goal': array([1.46511672, 0.89288092, 0.46417504])}, 'done': False}\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "{'state': {'observation': array([ 1.48928586e+00,  9.67088432e-01,  5.52030387e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -1.14390660e-04,  2.03933389e-03,  1.81892193e-02,\n",
      "        7.05507172e-04,  1.36487002e-07]), 'achieved_goal': array([1.48928586, 0.96708843, 0.55203039]), 'desired_goal': array([1.48928586, 0.96708843, 0.55203039])}, 'action': array([ 0.2858571 , -0.21752213,  0.76701676,  0.30114902]), 'reward': -1.0, 'next_state': {'observation': array([ 1.48797060e+00,  9.57740135e-01,  5.75750691e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -1.09275058e-03, -8.25240320e-03,  1.62594116e-02,\n",
      "        7.32061936e-04,  1.34296333e-04]), 'achieved_goal': array([1.4879706 , 0.95774014, 0.57575069]), 'desired_goal': array([1.48928586, 0.96708843, 0.55203039])}, 'done': False}\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "{'state': {'observation': array([ 1.48205488e+00,  9.39526429e-01,  6.17382378e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -1.94459713e-03, -7.91729905e-03,  8.99885171e-03,\n",
      "        2.51926368e-04,  1.41840626e-04]), 'achieved_goal': array([1.48205488, 0.93952643, 0.61738238]), 'desired_goal': array([1.48205488, 0.93952643, 0.61738238])}, 'action': array([0.71640602, 0.31770519, 0.75747922, 0.26476058]), 'reward': -1.0, 'next_state': {'observation': array([ 1.48321186e+00,  9.43115467e-01,  6.35861266e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -3.67532827e-04,  4.09910271e-03,  1.35194611e-02,\n",
      "        5.80494531e-05,  4.18878655e-04]), 'achieved_goal': array([1.48321186, 0.94311547, 0.63586127]), 'desired_goal': array([1.48205488, 0.93952643, 0.61738238])}, 'done': False}\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "{'state': {'observation': array([ 1.49079667e+00,  9.24526960e-01,  6.69073292e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  4.66021662e-03, -1.92482343e-02, -3.67761853e-04,\n",
      "        5.79994128e-04,  3.89365963e-04]), 'achieved_goal': array([1.49079667, 0.92452696, 0.66907329]), 'desired_goal': array([1.49079667, 0.92452696, 0.66907329])}, 'action': array([ 1.        , -0.85967548,  0.2342749 ,  0.02367639]), 'reward': -1.0, 'next_state': {'observation': array([ 1.49752811e+00,  8.90897108e-01,  6.68178967e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  4.58981050e-03, -2.40526601e-02, -7.53137424e-04,\n",
      "        3.98115738e-04,  5.58319562e-04]), 'achieved_goal': array([1.49752811, 0.89089711, 0.66817897]), 'desired_goal': array([1.49079667, 0.92452696, 0.66907329])}, 'done': False}\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "{'state': {'observation': array([ 1.48964527e+00,  7.88848910e-01,  7.07623229e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -3.89180383e-03, -8.08131486e-03,  6.39873949e-03,\n",
      "        7.09883632e-06,  5.08797279e-04]), 'achieved_goal': array([1.48964527, 0.78884891, 0.70762323]), 'desired_goal': array([1.48964527, 0.78884891, 0.70762323])}, 'action': array([ 0.40608568, -0.51329771,  0.61853044,  0.10206528]), 'reward': -1.0, 'next_state': {'observation': array([ 1.48444496e+00,  7.71447427e-01,  7.20173355e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -3.99187764e-03, -1.29434797e-02,  9.13264119e-03,\n",
      "        7.64543210e-05,  2.76010384e-04]), 'achieved_goal': array([1.48444496, 0.77144743, 0.72017336]), 'desired_goal': array([1.48964527, 0.78884891, 0.70762323])}, 'done': False}\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "{'state': {'observation': array([ 1.47233897e+00,  6.88944490e-01,  7.47577217e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -1.71681883e-03, -2.06770524e-02, -3.58410852e-04,\n",
      "       -4.82294980e-05,  3.86576450e-04]), 'achieved_goal': array([1.47233897, 0.68894449, 0.74757722]), 'desired_goal': array([1.47233897, 0.68894449, 0.74757722])}, 'action': array([ 0.62137674, -1.        ,  0.37272804,  0.69476751]), 'reward': -1.0, 'next_state': {'observation': array([ 1.46987296e+00,  6.58088597e-01,  7.48820280e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -2.32042679e-03, -2.14719713e-02,  9.48353373e-04,\n",
      "       -1.21369614e-04,  5.05527982e-04]), 'achieved_goal': array([1.46987296, 0.6580886 , 0.74882028]), 'desired_goal': array([1.47233897, 0.68894449, 0.74757722])}, 'done': False}\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "{'state': {'observation': array([ 1.42298253e+00,  4.91697870e-01,  7.38574641e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -9.58761332e-03, -1.35197889e-02,  6.86470870e-03,\n",
      "        4.22813645e-06,  6.01503577e-04]), 'achieved_goal': array([1.42298253, 0.49169787, 0.73857464]), 'desired_goal': array([1.42298253, 0.49169787, 0.73857464])}, 'action': array([-0.27691544, -0.22065632,  0.43754445,  0.29530804]), 'reward': -1.0, 'next_state': {'observation': array([ 1.41013480e+00,  4.84753973e-01,  7.51240929e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -8.24218657e-03, -3.45837270e-03,  9.38887501e-03,\n",
      "       -1.45973672e-06,  7.10348033e-04]), 'achieved_goal': array([1.4101348 , 0.48475397, 0.75124093]), 'desired_goal': array([1.42298253, 0.49169787, 0.73857464])}, 'done': False}\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "{'state': {'observation': array([1.33601638e+00, 4.93566452e-01, 8.75115656e-01, 0.00000000e+00,\n",
      "       0.00000000e+00, 5.06976045e-05, 1.23822124e-02, 9.16777263e-03,\n",
      "       1.58710212e-04, 4.62934197e-04]), 'achieved_goal': array([1.33601638, 0.49356645, 0.87511566]), 'desired_goal': array([1.33601638, 0.49356645, 0.87511566])}, 'action': array([0.51284968, 0.12488296, 0.37391482, 0.50719595]), 'reward': -1.0, 'next_state': {'observation': array([1.33955937e+00, 5.03573662e-01, 8.77352354e-01, 0.00000000e+00,\n",
      "       0.00000000e+00, 3.27563852e-03, 5.79633486e-03, 6.27954231e-05,\n",
      "       3.34672601e-04, 9.00386086e-05]), 'achieved_goal': array([1.33955937, 0.50357366, 0.87735235]), 'desired_goal': array([1.33601638, 0.49356645, 0.87511566])}, 'done': False}\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "{'state': {'observation': array([ 1.36380509e+00,  5.29671629e-01,  8.65586477e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  7.97371818e-03,  7.96004952e-03, -3.71149739e-03,\n",
      "        2.67361675e-04, -2.78270702e-05]), 'achieved_goal': array([1.36380509, 0.52967163, 0.86558648]), 'desired_goal': array([1.36380509, 0.52967163, 0.86558648])}, 'action': array([ 0.940694  , -0.28402626,  0.36705359,  0.54889769]), 'reward': -1.0, 'next_state': {'observation': array([ 1.37266521e+00,  5.28738339e-01,  8.59810677e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  4.97104551e-03, -2.11444797e-03, -4.22292086e-03,\n",
      "        4.70266825e-04,  1.84348207e-05]), 'achieved_goal': array([1.37266521, 0.52873834, 0.85981068]), 'desired_goal': array([1.36380509, 0.52967163, 0.86558648])}, 'done': False}\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "{'state': {'observation': array([ 1.37266521e+00,  5.28738339e-01,  8.59810677e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  4.97104551e-03, -2.11444797e-03, -4.22292086e-03,\n",
      "        4.70266825e-04,  1.84348207e-05]), 'achieved_goal': array([1.37266521, 0.52873834, 0.85981068]), 'desired_goal': array([1.37266521, 0.52873834, 0.85981068])}, 'action': array([ 0.33613508, -0.19253034, -0.29583493,  0.55468666]), 'reward': -1.0, 'next_state': {'observation': array([ 1.37392466e+00,  5.25784407e-01,  8.44741746e-01,  0.00000000e+00,\n",
      "        0.00000000e+00,  4.10174243e-03, -3.38818874e-03, -1.10296756e-02,\n",
      "        3.64758949e-04,  7.08413858e-06]), 'achieved_goal': array([1.37392466, 0.52578441, 0.84474175]), 'desired_goal': array([1.37266521, 0.52873834, 0.85981068])}, 'done': False}\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "*******\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DDPG + HER\n",
    "\n",
    "EPISODES = 1\n",
    "BATCH_SIZE = 64\n",
    "state_dim = 16\n",
    "action_dim = 4\n",
    "memory_size = 1_00\n",
    "replay_memory = ReplayMemory(memory_size, state_dim, action_dim)\n",
    "replay_memory.populate(env, 50)\n",
    "saved_models = {}\n",
    "count = 0\n",
    "ou = OUNoise(env.action_space)\n",
    "ddpg = DDPG(state_dim, action_dim)\n",
    "\n",
    "\n",
    "pbar = tqdm.tnrange(EPISODES, ncols='100%')\n",
    "for t_total in pbar:\n",
    "    \n",
    "    # episode begins here\n",
    "    transitions = []\n",
    "    achieved_goals = []\n",
    "    raw_state = env.reset()\n",
    "    state = np.concatenate((raw_state[\"observation\"], raw_state[\"achieved_goal\"], raw_state[\"desired_goal\"]), axis=0)\n",
    "    while True:\n",
    "        cuda_state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "        action = ddpg.get_actions(cuda_state).cpu()\n",
    "        action = ou.get_noisy_action(action[0].numpy())\n",
    "        raw_next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.concatenate((raw_next_state[\"observation\"], raw_next_state[\"achieved_goal\"], raw_next_state[\"desired_goal\"]), axis=0)\n",
    "        transitions.append({\"state\": raw_state, \"action\": action, \"reward\": reward, \"next_state\": raw_next_state, \"done\": done})\n",
    "        achieved_goals.append(raw_state[\"achieved_goal\"])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "        raw_state = raw_next_state\n",
    "        \n",
    "    for transition in transitions:\n",
    "        #replay_memory.add(transition[\"state\"], transition[\"action\"], transition[\"reward\"], transition[\"next_state\"], transition[\"done\"])\n",
    "        #print(sample_goals)\n",
    "        sample_goals = random.sample(achieved_goals, 10)\n",
    "        for sample_goal in sample_goals:\n",
    "            #print(sample_goal)\n",
    "            #print(transition)\n",
    "            transition[\"state\"][\"desired_goal\"] = sample_goal\n",
    "            transition[\"next_state\"][\"desired_goal\"] = sample_goal\n",
    "            if( np.array_equal(transition[\"state\"][\"desired_goal\"], transition[\"state\"][\"achieved_goal\"]) ):\n",
    "                print(transition)\n",
    "                print(\"*******\")\n",
    "            #print('-'*20)\n",
    "            #print(transition)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# state = env.reset()\n",
    "# state = np.concatenate((state[\"observation\"], state[\"achieved_goal\"], state[\"desired_goal\"]), axis=0)\n",
    "# pbar = tqdm.tnrange(TIMESTEPS, ncols='100%')\n",
    "# for t_total in pbar:\n",
    "#     cuda_state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "#     action = ddpg.get_actions(cuda_state).cpu()\n",
    "#     action = ou.get_noisy_action(action[0].numpy())\n",
    "#     next_state, reward, done, info = env.step(action)\n",
    "#     next_state = np.concatenate((next_state[\"observation\"], next_state[\"achieved_goal\"], next_state[\"desired_goal\"]), axis=0)\n",
    "#     replay_memory.add(state, action, reward, next_state, done)\n",
    "#     if t_total != 0 and t_total % 4 == 0:\n",
    "#         experience_batch = replay_memory.sample(BATCH_SIZE)\n",
    "#         ddpg.train_batch(experience_batch.states, experience_batch.actions, experience_batch.rewards, experience_batch.next_states, experience_batch.dones)\n",
    "    \n",
    "#     if t_total != 0 and t_total % 1_00_000 == 0:\n",
    "#         torch.save(ddpg.target_actor.state_dict(), \"mc_\" + str(count))\n",
    "#         count += 1\n",
    "#         print(\"saved\")\n",
    "            \n",
    "#     if done:\n",
    "#         state = env.reset()\n",
    "#         state = np.concatenate((state[\"observation\"], state[\"achieved_goal\"], state[\"desired_goal\"]), axis=0)\n",
    "#     else:\n",
    "#         state = next_state\n",
    "\n",
    "# torch.save(ddpg.target_actor.state_dict(), \"mc_\" + str(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7a74fc0eb43b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mreplay_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReplayMemory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreplay_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-e91ba7981e3a>\u001b[0m in \u001b[0;36mpopulate\u001b[1;34m(self, env, num_steps)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"observation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"achieved_goal\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"desired_goal\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\robotics\\robot_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\robotics\\fetch_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mgrip_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_site_xpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'robot0:grip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnsubsteps\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mgrip_velp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_site_xvelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'robot0:grip'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mrobot_qpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrobot_qvel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrobot_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_object\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "replay_memory = ReplayMemory(1000000, 16, 4)\n",
    "replay_memory.populate(env, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddpg = DDPG(16, 4)\n",
    "# ddpg.load_model(\"fetchreach_9\")\n",
    "# print(ddpg.load_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n",
      "{'observation': array([ 4.76456721e-01,  1.03230452e+00,  1.78120232e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -2.14947340e-03, -1.32529089e-03,  1.21152092e-04,\n",
      "        7.30808668e-08,  5.92609964e-05]), 'achieved_goal': array([0.47645672, 1.03230452, 0.17812023]), 'desired_goal': array([1.43235837, 0.65064789, 0.46270819])} -1.0 True {'is_success': 0.0, 'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    state = np.concatenate((state[\"observation\"], state[\"achieved_goal\"], state[\"desired_goal\"]), axis=0)\n",
    "    cuda_state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "    action = ddpg.target_actor(cuda_state)\n",
    "    action = action.cpu()\n",
    "    #print(action.detach().numpy()[0])\n",
    "    next_state, reward, done, info = env.step(action.detach().numpy()[0])\n",
    "    state = np.concatenate((next_state[\"observation\"], next_state[\"achieved_goal\"], next_state[\"desired_goal\"]), axis=0)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(next_state, reward, done, info)\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8589934592"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0).total_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(25000, 25000).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ReplayMemory(100, 16, 4)\n",
    "r.populate(env, 10)\n",
    "\n",
    "sam_state = r.sample(4).states[1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3490719e+00 7.5555569e-01 5.3789562e-01 0.0000000e+00 0.0000000e+00\n",
      " 8.2962792e-03 3.7673535e-03 2.8055542e-06 7.5175289e-05 7.1640839e-05\n",
      " 1.3490719e+00 7.5555569e-01 5.3789562e-01 1.3344340e+00 7.6176006e-01\n",
      " 4.4385520e-01]\n",
      "[1.334434   0.76176006 0.4438552 ]\n"
     ]
    }
   ],
   "source": [
    "print(sam_state)\n",
    "print(sam_state[-3:])\n",
    "sam_state[-3:] = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.3490719e+00, 7.5555569e-01, 5.3789562e-01, 0.0000000e+00,\n",
       "       0.0000000e+00, 8.2962792e-03, 3.7673535e-03, 2.8055542e-06,\n",
       "       7.5175289e-05, 7.1640839e-05, 1.3490719e+00, 7.5555569e-01,\n",
       "       5.3789562e-01, 1.0000000e+00, 2.0000000e+00, 3.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
