{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "import gym\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import more_itertools as mitt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [12, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v1')\n",
    "torch.cuda.current_device()\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "  device = \"cuda:0\" \n",
    "else:  \n",
    "  device = \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = namedtuple(\n",
    "    'Batch', ('states', 'actions', 'rewards', 'next_states', 'dones')\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, max_size, state_size, action_size):\n",
    "        self.max_size = max_size\n",
    "        self.state_size = state_size\n",
    "        self.states = torch.empty((max_size, state_size), device=device)\n",
    "        self.actions = torch.empty((max_size, action_size), device=device)\n",
    "        self.rewards = torch.empty((max_size, 1), device=device)\n",
    "        self.next_states = torch.empty((max_size, state_size), device=device)\n",
    "        self.dones = torch.empty((max_size, 1), dtype=torch.bool, device=device)\n",
    "        self.idx = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states[self.idx] = torch.tensor(state, device=device)\n",
    "        self.actions[self.idx] = torch.tensor(action, device=device)\n",
    "        self.rewards[self.idx] = torch.tensor(reward, device=device)\n",
    "        self.next_states[self.idx] = torch.tensor(next_state, device=device)\n",
    "        self.dones[self.idx] = torch.tensor(done, device=device)\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size) -> Batch:\n",
    "        if self.size <= batch_size:\n",
    "            sample_indices = np.random.choice(self.size, self.size, replace=False)\n",
    "        else:\n",
    "            sample_indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        batch = Batch(\n",
    "                        states = self.states[ sample_indices ,:],\n",
    "                        actions = self.actions[ sample_indices ,:],\n",
    "                        rewards = self.rewards[ sample_indices ,:],\n",
    "                        next_states = self.next_states[ sample_indices ,:],\n",
    "                        dones = self.dones[ sample_indices ,:]\n",
    "                    )\n",
    "        return batch\n",
    "\n",
    "    def populate(self, env, num_steps):\n",
    "        state = env.reset()\n",
    "        state = np.concatenate((state[\"observation\"], state[\"achieved_goal\"], state[\"desired_goal\"]), axis=0)\n",
    "        for i in range(num_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.concatenate((next_state[\"observation\"], next_state[\"achieved_goal\"], next_state[\"desired_goal\"]), axis=0)\n",
    "            self.add(state, action, reward, next_state, done)\n",
    "            if i != 0 and i%10000 == 0:\n",
    "                print(i)\n",
    "            if done:\n",
    "                if not('TimeLimit.truncated' in info):\n",
    "                    print(next_state, reward, done, info, i)\n",
    "                state = env.reset()\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers, units=256):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.units = units\n",
    "        \n",
    "        self.layers = nn.ModuleList([nn.Linear(self.input_size, self.units)])\n",
    "        self.layers.extend([ nn.Linear(self.units, self.units) for i in range(1, self.hidden_layers) ])\n",
    "        self.layers.append(nn.Linear(self.units, self.output_size))\n",
    "    \n",
    "    def forward(self, states):\n",
    "        vals = states\n",
    "        for layer_index in range(len(self.layers) - 1):\n",
    "            vals = F.relu(self.layers[layer_index](vals))\n",
    "        vals = torch.tanh(self.layers[layer_index + 1](vals))\n",
    "        return vals\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_layers, units=256):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.units = units\n",
    "        \n",
    "        self.layers = nn.ModuleList([nn.Linear(self.input_size, self.units)])\n",
    "        self.layers.extend([ nn.Linear(self.units, self.units) for i in range(1, self.hidden_layers) ])\n",
    "        self.layers.append(nn.Linear(self.units, self.output_size))\n",
    "    \n",
    "    def forward(self, states, actions):\n",
    "        vals = torch.cat([states, actions], 1)\n",
    "        for layer_index in range(len(self.layers) - 1):\n",
    "            vals = F.relu(self.layers[layer_index](vals))\n",
    "        vals = self.layers[layer_index + 1](vals)\n",
    "        return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise(object):\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[0]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_noisy_action(self, action, t=0): \n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, actor_learning_rate=0.0001, critic_learning_rate=0.0001, gamma=0.99, tau=0.005):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.actor_lr = actor_learning_rate\n",
    "        self.critic_lr = critic_learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.actor = Actor(self.state_size, self.action_size, 5)\n",
    "        self.target_actor = Actor(self.state_size, self.action_size, 5)\n",
    "        \n",
    "        self.critic = Critic(self.state_size + self.action_size, self.action_size, 5)\n",
    "        self.target_critic = Critic(self.state_size + self.action_size, self.action_size, 5)\n",
    "    \n",
    "        self.actor.to(device)\n",
    "        self.target_actor.to(device)\n",
    "        self.critic.to(device)\n",
    "        self.target_critic.to(device)\n",
    "        self.update_weights(1)\n",
    "        \n",
    "        self.critic_loss_method  = nn.MSELoss()\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "        # print(next(self.actor.parameters()).is_cuda)\n",
    "    \n",
    "    def update_weights(self, tau):\n",
    "        for target_weights, weights in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_weights.data.copy_(weights.data * tau + target_weights.data * (1.0 - tau))\n",
    "        \n",
    "        for target_weights, weights in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_weights.data.copy_(weights.data * tau + target_weights.data * (1.0 - tau))\n",
    "            \n",
    "    def get_actions(self, states):\n",
    "        return self.actor.forward(states).detach()\n",
    "\n",
    "    def train_batch(self, states, actions, rewards, next_states, dones):\n",
    "        #print(dones)\n",
    "        \n",
    "        Q_vals = self.critic(states, actions)\n",
    "        next_actions = self.target_actor(next_states)\n",
    "        next_actions = next_actions.detach()\n",
    "        Q_dash = self.target_critic(next_states, next_actions)\n",
    "        dones = torch.logical_not(dones).float()\n",
    "        Q_dash = rewards + self.gamma * Q_dash * dones\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss = self.critic_loss_method(Q_vals, Q_dash)\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss = - self.critic(states, self.actor.forward(states)).mean()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.update_weights(self.tau)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        self.target_actor.load_state_dict(torch.load(model_path))\n",
    "        self.target_actor.eval()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cd06676a4245faa77ccb00ccfaafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=1500000), HTML(value='')), layout=Layout(disp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-fbf53868b882>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt_total\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_total\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mexperience_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperience_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperience_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt_total\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_total\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1_00_000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-a77689a5b380>\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_loss_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_dash\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mcritic_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TIMESTEPS = 15_00_000\n",
    "BATCH_SIZE = 64\n",
    "state_dim = 16\n",
    "action_dim = 4\n",
    "memory_size = 1_00_000\n",
    "replay_memory = ReplayMemory(memory_size, state_dim, action_dim)\n",
    "replay_memory.populate(env, 50000)\n",
    "saved_models = {}\n",
    "count = 0\n",
    "ou = OUNoise(env.action_space)\n",
    "ddpg = DDPG(state_dim, action_dim)\n",
    "\n",
    "state = env.reset()\n",
    "state = np.concatenate((state[\"observation\"], state[\"achieved_goal\"], state[\"desired_goal\"]), axis=0)\n",
    "pbar = tqdm.tnrange(TIMESTEPS, ncols='100%')\n",
    "for t_total in pbar:\n",
    "    cuda_state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "    action = ddpg.get_actions(cuda_state).cpu()\n",
    "    action = ou.get_noisy_action(action[0].numpy())\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = np.concatenate((next_state[\"observation\"], next_state[\"achieved_goal\"], next_state[\"desired_goal\"]), axis=0)\n",
    "    replay_memory.add(state, action, reward, next_state, done)\n",
    "    if t_total != 0 and t_total % 4 == 0:\n",
    "        experience_batch = replay_memory.sample(BATCH_SIZE)\n",
    "        ddpg.train_batch(experience_batch.states, experience_batch.actions, experience_batch.rewards, experience_batch.next_states, experience_batch.dones)\n",
    "    \n",
    "    if t_total != 0 and t_total % 1_00_000 == 0:\n",
    "        torch.save(ddpg.target_actor.state_dict(), \"mc_\" + str(count))\n",
    "        count += 1\n",
    "        print(\"saved\")\n",
    "            \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        state = np.concatenate((state[\"observation\"], state[\"achieved_goal\"], state[\"desired_goal\"]), axis=0)\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "torch.save(ddpg.target_actor.state_dict(), \"mc_\" + str(count))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cc50e79bb1408b9cde046a40c514bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=10000), HTML(value='')), layout=Layout(displa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "saved\n",
      "saved\n",
      "saved\n",
      "saved\n",
      "saved\n",
      "saved\n",
      "saved\n",
      "saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DDPG + HER\n",
    "\n",
    "EPISODES = 10_000\n",
    "BATCH_SIZE = 64\n",
    "state_dim = 16\n",
    "action_dim = 4\n",
    "memory_size = 1_00_000\n",
    "replay_memory = ReplayMemory(memory_size, state_dim, action_dim)\n",
    "replay_memory.populate(env, 50_000)\n",
    "saved_models = {}\n",
    "count = 0\n",
    "ou = OUNoise(env.action_space)\n",
    "ddpg = DDPG(state_dim, action_dim)\n",
    "\n",
    "\n",
    "pbar = tqdm.tnrange(EPISODES, ncols='100%')\n",
    "for t_total in pbar:\n",
    "    \n",
    "    # episode begins here\n",
    "    transitions = []\n",
    "    achieved_goals = []\n",
    "    raw_state = env.reset()\n",
    "    state = np.concatenate((raw_state[\"observation\"], raw_state[\"achieved_goal\"], raw_state[\"desired_goal\"]), axis=0)\n",
    "    while True:\n",
    "        cuda_state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "        action = ddpg.get_actions(cuda_state).cpu()\n",
    "        action = ou.get_noisy_action(action[0].numpy())\n",
    "        raw_next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.concatenate((raw_next_state[\"observation\"], raw_next_state[\"achieved_goal\"], raw_next_state[\"desired_goal\"]), axis=0)\n",
    "        transitions.append({\"state\": raw_state, \"action\": action, \"reward\": reward, \"next_state\": raw_next_state, \"done\": done})\n",
    "        achieved_goals.append(raw_state[\"achieved_goal\"])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "        raw_state = raw_next_state\n",
    "        \n",
    "    for transition in transitions:\n",
    "        temp_state = np.concatenate((transition[\"state\"][\"observation\"], transition[\"state\"][\"achieved_goal\"], transition[\"state\"][\"desired_goal\"]), axis=0)\n",
    "        temp_next_state = np.concatenate((transition[\"next_state\"][\"observation\"], transition[\"next_state\"][\"achieved_goal\"], transition[\"next_state\"][\"desired_goal\"]), axis=0)\n",
    "        replay_memory.add(temp_state, transition[\"action\"], transition[\"reward\"], temp_next_state, transition[\"done\"])\n",
    "        #print(sample_goals)\n",
    "        sample_goals = random.sample(achieved_goals, 10)\n",
    "        for sample_goal in sample_goals:\n",
    "            transition[\"state\"][\"desired_goal\"] = sample_goal\n",
    "            transition[\"next_state\"][\"desired_goal\"] = sample_goal\n",
    "            if( np.array_equal(transition[\"state\"][\"desired_goal\"], transition[\"state\"][\"achieved_goal\"]) ):\n",
    "                transition[\"done\"] = True\n",
    "                transition[\"reward\"] = 0.0\n",
    "            \n",
    "            temp_state = np.concatenate((transition[\"state\"][\"observation\"], transition[\"state\"][\"achieved_goal\"], transition[\"state\"][\"desired_goal\"]), axis=0)\n",
    "            temp_next_state = np.concatenate((transition[\"next_state\"][\"observation\"], transition[\"next_state\"][\"achieved_goal\"], transition[\"next_state\"][\"desired_goal\"]), axis=0)\n",
    "            replay_memory.add(temp_state, transition[\"action\"], transition[\"reward\"], temp_next_state, transition[\"done\"])\n",
    "    \n",
    "    for i in range(20):\n",
    "        experience_batch = replay_memory.sample(BATCH_SIZE)\n",
    "        ddpg.train_batch(experience_batch.states, experience_batch.actions, experience_batch.rewards, experience_batch.next_states, experience_batch.dones)\n",
    "        \n",
    "    \n",
    "    if t_total != 0 and t_total % 1_000 == 0:\n",
    "        torch.save(ddpg.target_actor.state_dict(), \"her_ta_\" + str(count))\n",
    "        torch.save(ddpg.actor.state_dict(), \"her_a_\" + str(count))\n",
    "        torch.save(ddpg.target_critic.state_dict(), \"her_tc_\" + str(count))\n",
    "        torch.save(ddpg.critic.state_dict(), \"her_c_\" + str(count))\n",
    "        count += 1\n",
    "        print(\"saved\")\n",
    "        \n",
    "torch.save(ddpg.target_actor.state_dict(), \"her_ta_\" + str(count))\n",
    "torch.save(ddpg.actor.state_dict(), \"her_a_\" + str(count))\n",
    "torch.save(ddpg.target_critic.state_dict(), \"her_tc_\" + str(count))\n",
    "torch.save(ddpg.critic.state_dict(), \"her_c_\" + str(count))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404844f5a5e246698f6424073f882d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=10000), HTML(value='')), layout=Layout(displa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-8e9a6061610e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mtemp_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"observation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"achieved_goal\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"desired_goal\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mtemp_next_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"next_state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"observation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"next_state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"achieved_goal\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"next_state\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"desired_goal\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mreplay_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"action\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"reward\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_next_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"done\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-659494905d32>\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPISODES = 10_000\n",
    "BATCH_SIZE = 64\n",
    "state_dim = 16\n",
    "action_dim = 4\n",
    "memory_size = 1_00_000\n",
    "#replay_memory = ReplayMemory(memory_size, state_dim, action_dim)\n",
    "#replay_memory.populate(env, 50_000)\n",
    "saved_models = {}\n",
    "count = 9\n",
    "# ou = OUNoise(env.action_space)\n",
    "# ddpg = DDPG(state_dim, action_dim)\n",
    "\n",
    "\n",
    "pbar = tqdm.tnrange(EPISODES, ncols='100%')\n",
    "for t_total in pbar:\n",
    "    \n",
    "    # episode begins here\n",
    "    transitions = []\n",
    "    achieved_goals = []\n",
    "    raw_state = env.reset()\n",
    "    state = np.concatenate((raw_state[\"observation\"], raw_state[\"achieved_goal\"], raw_state[\"desired_goal\"]), axis=0)\n",
    "    while True:\n",
    "        cuda_state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "        action = ddpg.get_actions(cuda_state).cpu()\n",
    "        action = ou.get_noisy_action(action[0].numpy())\n",
    "        raw_next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.concatenate((raw_next_state[\"observation\"], raw_next_state[\"achieved_goal\"], raw_next_state[\"desired_goal\"]), axis=0)\n",
    "        transitions.append({\"state\": raw_state, \"action\": action, \"reward\": reward, \"next_state\": raw_next_state, \"done\": done})\n",
    "        achieved_goals.append(raw_state[\"achieved_goal\"])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "        raw_state = raw_next_state\n",
    "        \n",
    "    for transition in transitions:\n",
    "        temp_state = np.concatenate((transition[\"state\"][\"observation\"], transition[\"state\"][\"achieved_goal\"], transition[\"state\"][\"desired_goal\"]), axis=0)\n",
    "        temp_next_state = np.concatenate((transition[\"next_state\"][\"observation\"], transition[\"next_state\"][\"achieved_goal\"], transition[\"next_state\"][\"desired_goal\"]), axis=0)\n",
    "        replay_memory.add(temp_state, transition[\"action\"], transition[\"reward\"], temp_next_state, transition[\"done\"])\n",
    "        #print(sample_goals)\n",
    "        sample_goals = random.sample(achieved_goals, 10)\n",
    "        for sample_goal in sample_goals:\n",
    "            transition[\"state\"][\"desired_goal\"] = sample_goal\n",
    "            transition[\"next_state\"][\"desired_goal\"] = sample_goal\n",
    "            if( np.array_equal(transition[\"state\"][\"desired_goal\"], transition[\"state\"][\"achieved_goal\"]) ):\n",
    "                transition[\"done\"] = True\n",
    "                transition[\"reward\"] = 0.0\n",
    "            \n",
    "            temp_state = np.concatenate((transition[\"state\"][\"observation\"], transition[\"state\"][\"achieved_goal\"], transition[\"state\"][\"desired_goal\"]), axis=0)\n",
    "            temp_next_state = np.concatenate((transition[\"next_state\"][\"observation\"], transition[\"next_state\"][\"achieved_goal\"], transition[\"next_state\"][\"desired_goal\"]), axis=0)\n",
    "            replay_memory.add(temp_state, transition[\"action\"], transition[\"reward\"], temp_next_state, transition[\"done\"])\n",
    "    \n",
    "    for i in range(20):\n",
    "        experience_batch = replay_memory.sample(BATCH_SIZE)\n",
    "        ddpg.train_batch(experience_batch.states, experience_batch.actions, experience_batch.rewards, experience_batch.next_states, experience_batch.dones)\n",
    "        \n",
    "    \n",
    "    if t_total != 0 and t_total % 1_000 == 0:\n",
    "        torch.save(ddpg.target_actor.state_dict(), \"her_ta_\" + str(count))\n",
    "        torch.save(ddpg.actor.state_dict(), \"her_a_\" + str(count))\n",
    "        torch.save(ddpg.target_critic.state_dict(), \"her_tc_\" + str(count))\n",
    "        torch.save(ddpg.critic.state_dict(), \"her_c_\" + str(count))\n",
    "        count += 1\n",
    "        print(\"saved\")\n",
    "        \n",
    "torch.save(ddpg.target_actor.state_dict(), \"her_ta_\" + str(count))\n",
    "torch.save(ddpg.actor.state_dict(), \"her_a_\" + str(count))\n",
    "torch.save(ddpg.target_critic.state_dict(), \"her_tc_\" + str(count))\n",
    "torch.save(ddpg.critic.state_dict(), \"her_c_\" + str(count))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n",
      "{'observation': array([ 4.76456721e-01,  1.03230452e+00,  1.78120232e-01,  0.00000000e+00,\n",
      "        0.00000000e+00, -2.14947340e-03, -1.32529089e-03,  1.21152092e-04,\n",
      "        7.30808668e-08,  5.92609964e-05]), 'achieved_goal': array([0.47645672, 1.03230452, 0.17812023]), 'desired_goal': array([1.43235837, 0.65064789, 0.46270819])} -1.0 True {'is_success': 0.0, 'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    state = np.concatenate((state[\"observation\"], state[\"achieved_goal\"], state[\"desired_goal\"]), axis=0)\n",
    "    cuda_state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "    action = ddpg.target_actor(cuda_state)\n",
    "    action = action.cpu()\n",
    "    #print(action.detach().numpy()[0])\n",
    "    next_state, reward, done, info = env.step(action.detach().numpy()[0])\n",
    "    state = np.concatenate((next_state[\"observation\"], next_state[\"achieved_goal\"], next_state[\"desired_goal\"]), axis=0)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        print(next_state, reward, done, info)\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8589934592"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0).total_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(25000, 25000).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ReplayMemory(100, 16, 4)\n",
    "r.populate(env, 10)\n",
    "\n",
    "sam_state = r.sample(4).states[1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.3490719e+00 7.5555569e-01 5.3789562e-01 0.0000000e+00 0.0000000e+00\n",
      " 8.2962792e-03 3.7673535e-03 2.8055542e-06 7.5175289e-05 7.1640839e-05\n",
      " 1.3490719e+00 7.5555569e-01 5.3789562e-01 1.3344340e+00 7.6176006e-01\n",
      " 4.4385520e-01]\n",
      "[1.334434   0.76176006 0.4438552 ]\n"
     ]
    }
   ],
   "source": [
    "print(sam_state)\n",
    "print(sam_state[-3:])\n",
    "sam_state[-3:] = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.3490719e+00, 7.5555569e-01, 5.3789562e-01, 0.0000000e+00,\n",
       "       0.0000000e+00, 8.2962792e-03, 3.7673535e-03, 2.8055542e-06,\n",
       "       7.5175289e-05, 7.1640839e-05, 1.3490719e+00, 7.5555569e-01,\n",
       "       5.3789562e-01, 1.0000000e+00, 2.0000000e+00, 3.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
